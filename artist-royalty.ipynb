{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Artist Royalty Data Quality Project\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a comprehensive data quality assessment and analysis of artist royalty transactions for a music rights organization. The project includes synthetic data generation, quality issue identification, data cleaning processes, anomaly detection, and business intelligence visualizations.\n",
    "\n",
    "### Project Objectives:\n",
    "- **Data Quality Assessment**: Identify and quantify data quality issues in royalty transactions\n",
    "- **Data Cleaning & Standardization**: Implement robust cleaning procedures for production use\n",
    "- **Anomaly Detection**: Identify unusual payment patterns that may require investigation\n",
    "- **Business Intelligence**: Generate actionable insights for stakeholders\n",
    "- **Process Documentation**: Create reproducible workflows for ongoing data quality monitoring\n",
    "\n",
    "### Key Findings Preview:\n",
    "- Generated dataset: 200,000 transactions across multiple licensing channels\n",
    "- Data quality issues: ~5% missing values, ~2% duplicate records\n",
    "- Anomalies detected: Statistical outliers in royalty payments\n",
    "- Business insights: Revenue distribution analysis by artist, channel, and region\n",
    "\n",
    "---\n",
    "\n",
    "**Prepared by**: Data Quality Team  \n",
    "**Date**: 2024  \n",
    "**Version**: 1.0  \n",
    "**Status**: Final Report\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Environment Setup & Dependencies\n",
    "\n",
    "This section imports all necessary libraries and configures the environment for data generation, analysis, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Statistical analysis and anomaly detection\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize Faker for realistic data generation\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(\"Environment configured for reproducible results\")\n",
    "print(\"Visualization settings optimized\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Synthetic Data Generation\n",
    "\n",
    "### 2.1 Data Sources & Methodology\n",
    "\n",
    "This section generates realistic royalty transaction data that simulates real-world scenarios commonly encountered in music rights management:\n",
    "\n",
    "- **Sample Size**: 200,000 transactions over 2 years\n",
    "- **Quality Issues**: Intentionally introduced missing values (5%) and duplicates (2%)\n",
    "- **Business Logic**: Realistic royalty amounts based on licensing channels and geographic regions\n",
    "- **Temporal Patterns**: Seasonal variations in music consumption and payment cycles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_royalty_data(n_records=200000):\n",
    "    \"\"\"\n",
    "    Generate synthetic royalty transaction data with realistic business patterns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_records : int\n",
    "        Number of records to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Generated royalty data with intentional quality issues\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define realistic business parameters\n",
    "    artists = [f\"Artist_{i:04d}\" for i in range(1, 501)]  # 500 unique artists\n",
    "    artist_names = [fake.name() for _ in range(500)]\n",
    "    \n",
    "    licensing_channels = ['Streaming', 'Radio', 'TV/Film', 'Digital Download', \n",
    "                         'Physical Sales', 'Live Performance', 'Sync License']\n",
    "    \n",
    "    regions = ['North America', 'Europe', 'Asia Pacific', 'Latin America', \n",
    "               'Middle East', 'Africa', 'Global']\n",
    "    \n",
    "    payment_statuses = ['Paid', 'Pending', 'Processing', 'Hold', 'Disputed']\n",
    "    \n",
    "    # Channel-specific royalty multipliers (realistic industry rates)\n",
    "    channel_multipliers = {\n",
    "        'Streaming': 1.0,\n",
    "        'Radio': 2.5,\n",
    "        'TV/Film': 5.0,\n",
    "        'Digital Download': 3.0,\n",
    "        'Physical Sales': 4.0,\n",
    "        'Live Performance': 1.5,\n",
    "        'Sync License': 8.0\n",
    "    }\n",
    "    \n",
    "    print(\"Generating base dataset...\")\n",
    "    \n",
    "    # Generate base data\n",
    "    data = []\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    end_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(n_records):\n",
    "        # Generate transaction details\n",
    "        artist_idx = np.random.randint(0, len(artists))\n",
    "        artist_id = artists[artist_idx]\n",
    "        artist_name = artist_names[artist_idx]\n",
    "        \n",
    "        # Generate song details\n",
    "        song_id = f\"SONG_{np.random.randint(1, 10001):05d}\"\n",
    "        song_title = fake.sentence(nb_words=3).replace('.', '')\n",
    "        \n",
    "        # Generate licensing and geographic info\n",
    "        channel = np.random.choice(licensing_channels, p=[0.4, 0.15, 0.1, 0.15, 0.05, 0.1, 0.05])\n",
    "        region = np.random.choice(regions, p=[0.3, 0.25, 0.2, 0.1, 0.05, 0.05, 0.05])\n",
    "        \n",
    "        # Generate realistic royalty amounts based on channel\n",
    "        base_amount = np.random.exponential(scale=50)  # Exponential distribution for royalties\n",
    "        royalty_amount = base_amount * channel_multipliers[channel]\n",
    "        \n",
    "        # Add some noise and round to 2 decimal places\n",
    "        royalty_amount = round(royalty_amount * (0.8 + 0.4 * np.random.random()), 2)\n",
    "        \n",
    "        # Generate payment details\n",
    "        payment_status = np.random.choice(payment_statuses, p=[0.7, 0.15, 0.08, 0.05, 0.02])\n",
    "        \n",
    "        # Generate distribution date (with seasonal patterns)\n",
    "        days_from_start = np.random.randint(0, (end_date - start_date).days)\n",
    "        distribution_date = start_date + timedelta(days=days_from_start)\n",
    "        \n",
    "        # Create transaction record\n",
    "        record = {\n",
    "            'transaction_id': f\"TXN_{i+1:08d}\",\n",
    "            'artist_id': artist_id,\n",
    "            'artist_name': artist_name,\n",
    "            'song_id': song_id,\n",
    "            'song_title': song_title,\n",
    "            'royalty_amount': royalty_amount,\n",
    "            'distribution_date': distribution_date,\n",
    "            'payment_status': payment_status,\n",
    "            'licensing_channel': channel,\n",
    "            'region': region\n",
    "        }\n",
    "        \n",
    "        data.append(record)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 50000 == 0:\n",
    "            print(f\"Generated {i+1:,} records...\")\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"Starting royalty data generation...\")\n",
    "df_raw = generate_royalty_data(200000)\n",
    "print(f\"Generated {len(df_raw):,} royalty transactions\")\n",
    "print(f\"Date range: {df_raw['distribution_date'].min()} to {df_raw['distribution_date'].max()}\")\n",
    "print(f\"Total royalty value: ${df_raw['royalty_amount'].sum():,.2f}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df_raw.info())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Introducing Data Quality Issues\n",
    "\n",
    "To simulate real-world conditions, we intentionally introduce data quality issues that commonly occur in production systems:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_data_quality_issues(df, missing_rate=0.05, duplicate_rate=0.02):\n",
    "    \"\"\"\n",
    "    Introduce realistic data quality issues to simulate production data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Clean dataset\n",
    "    missing_rate : float\n",
    "        Percentage of values to make missing (default 5%)\n",
    "    duplicate_rate : float\n",
    "        Percentage of records to duplicate (default 2%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataset with introduced quality issues\n",
    "    \"\"\"\n",
    "    \n",
    "    df_with_issues = df.copy()\n",
    "    \n",
    "    print(\"Introducing data quality issues...\")\n",
    "    \n",
    "    # 1. Introduce missing values (5% across various columns)\n",
    "    columns_for_missing = ['artist_name', 'song_title', 'royalty_amount', 'payment_status', 'region']\n",
    "    \n",
    "    for col in columns_for_missing:\n",
    "        # Calculate number of missing values for this column\n",
    "        n_missing = int(len(df) * missing_rate / len(columns_for_missing))\n",
    "        \n",
    "        # Randomly select indices to set as missing\n",
    "        missing_indices = np.random.choice(df.index, size=n_missing, replace=False)\n",
    "        df_with_issues.loc[missing_indices, col] = np.nan\n",
    "        \n",
    "        print(f\"   → Introduced {n_missing:,} missing values in '{col}'\")\n",
    "    \n",
    "    # 2. Introduce duplicate records (2%)\n",
    "    n_duplicates = int(len(df) * duplicate_rate)\n",
    "    \n",
    "    # Randomly select records to duplicate\n",
    "    duplicate_indices = np.random.choice(df.index, size=n_duplicates, replace=False)\n",
    "    duplicate_records = df_with_issues.loc[duplicate_indices].copy()\n",
    "    \n",
    "    # Modify transaction_ids for duplicates to avoid primary key conflicts\n",
    "    duplicate_records['transaction_id'] = duplicate_records['transaction_id'] + '_DUP'\n",
    "    \n",
    "    # Append duplicates to the dataset\n",
    "    df_with_issues = pd.concat([df_with_issues, duplicate_records], ignore_index=True)\n",
    "    \n",
    "    print(f\"   → Introduced {n_duplicates:,} duplicate records\")\n",
    "    \n",
    "    # 3. Introduce some formatting inconsistencies\n",
    "    # Random case issues in artist names\n",
    "    case_issues_idx = np.random.choice(df_with_issues.index, size=1000, replace=False)\n",
    "    for idx in case_issues_idx:\n",
    "        if pd.notna(df_with_issues.loc[idx, 'artist_name']):\n",
    "            df_with_issues.loc[idx, 'artist_name'] = df_with_issues.loc[idx, 'artist_name'].upper()\n",
    "    \n",
    "    # Random whitespace issues\n",
    "    whitespace_idx = np.random.choice(df_with_issues.index, size=500, replace=False)\n",
    "    for idx in whitespace_idx:\n",
    "        if pd.notna(df_with_issues.loc[idx, 'song_title']):\n",
    "            df_with_issues.loc[idx, 'song_title'] = '  ' + df_with_issues.loc[idx, 'song_title'] + '  '\n",
    "    \n",
    "    print(f\"   → Introduced formatting inconsistencies (case, whitespace)\")\n",
    "    \n",
    "    return df_with_issues\n",
    "\n",
    "# Apply data quality issues\n",
    "df_dirty = introduce_data_quality_issues(df_raw)\n",
    "\n",
    "print(f\"\\nDataset Statistics After Quality Issues:\")\n",
    "print(f\"   • Total records: {len(df_dirty):,}\")\n",
    "print(f\"   • Original records: {len(df_raw):,}\")\n",
    "print(f\"   • Added duplicates: {len(df_dirty) - len(df_raw):,}\")\n",
    "\n",
    "# Check missing values\n",
    "missing_summary = df_dirty.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df_dirty) * 100).round(2)\n",
    "\n",
    "print(f\"\\nMissing Values Summary:\")\n",
    "for col in missing_summary.index:\n",
    "    if missing_summary[col] > 0:\n",
    "        print(f\"   • {col}: {missing_summary[col]:,} ({missing_pct[col]}%)\")\n",
    "\n",
    "# Display first few records\n",
    "print(f\"\\nSample of Generated Data:\")\n",
    "print(df_dirty.head(10))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Data Quality Assessment & Profiling\n",
    "\n",
    "### 3.1 Initial Data Profiling\n",
    "\n",
    "Before cleaning, we perform comprehensive data profiling to understand the scope of quality issues and establish baseline metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_data_profile(df, title=\"Dataset Profile\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality profile\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"{title}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Basic Statistics:\")\n",
    "    print(f\"   • Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"   • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"   • Date range: {df['distribution_date'].min()} to {df['distribution_date'].max()}\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    print(f\"\\nMissing Values Analysis:\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pcts = (missing_counts / len(df) * 100).round(2)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if missing_counts[col] > 0:\n",
    "            print(f\"   • {col}: {missing_counts[col]:,} ({missing_pcts[col]}%)\")\n",
    "    \n",
    "    total_missing = missing_counts.sum()\n",
    "    total_values = df.shape[0] * df.shape[1]\n",
    "    print(f\"   • Total missing: {total_missing:,} / {total_values:,} ({(total_missing/total_values*100):.2f}%)\")\n",
    "    \n",
    "    # Duplicate analysis\n",
    "    print(f\"\\nDuplicate Analysis:\")\n",
    "    \n",
    "    # Check for exact duplicates (excluding transaction_id)\n",
    "    cols_for_dup_check = [col for col in df.columns if col != 'transaction_id']\n",
    "    duplicates = df.duplicated(subset=cols_for_dup_check, keep='first')\n",
    "    n_duplicates = duplicates.sum()\n",
    "    \n",
    "    print(f\"   • Exact duplicates (excl. transaction_id): {n_duplicates:,} ({(n_duplicates/len(df)*100):.2f}%)\")\n",
    "    \n",
    "    # Check transaction_id duplicates\n",
    "    txn_duplicates = df['transaction_id'].duplicated().sum()\n",
    "    print(f\"   • Transaction ID duplicates: {txn_duplicates:,}\")\n",
    "    \n",
    "    # Cardinality analysis\n",
    "    print(f\"\\nCardinality Analysis:\")\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or col in ['artist_id', 'song_id']:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"   • {col}: {unique_count:,} unique values\")\n",
    "    \n",
    "    # Data type analysis\n",
    "    print(f\"\\nData Types:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"   • {col}: {df[col].dtype}\")\n",
    "    \n",
    "    # Statistical summary for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric Data Summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'missing_values': missing_counts.to_dict(),\n",
    "        'duplicates': n_duplicates,\n",
    "        'cardinality': {col: df[col].nunique() for col in df.columns}\n",
    "    }\n",
    "\n",
    "# Profile the dirty dataset\n",
    "profile_dirty = comprehensive_data_profile(df_dirty, \"Initial Data Quality Profile\")\n",
    "\n",
    "# Visualize missing values pattern\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_matrix = df_dirty.isnull()\n",
    "\n",
    "# Create heatmap of missing values (sample for visualization)\n",
    "sample_size = min(1000, len(df_dirty))\n",
    "sample_idx = np.random.choice(df_dirty.index, sample_size, replace=False)\n",
    "sample_missing = missing_matrix.loc[sample_idx]\n",
    "\n",
    "sns.heatmap(sample_missing, yticklabels=False, cbar=True, cmap='viridis_r')\n",
    "plt.title('Missing Values Pattern (Sample of 1,000 Records)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Records')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Missing values by column\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_counts = df_dirty.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "sns.barplot(x=missing_counts.values, y=missing_counts.index, palette='viridis')\n",
    "plt.title('Missing Values Count by Column')\n",
    "plt.xlabel('Number of Missing Values')\n",
    "plt.ylabel('Columns')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Data Cleaning & Standardization\n",
    "\n",
    "### 4.1 Cleaning Strategy\n",
    "\n",
    "Our data cleaning approach follows industry best practices for financial and royalty data:\n",
    "\n",
    "1. **Missing Value Treatment**: Context-aware imputation based on business rules\n",
    "2. **Duplicate Removal**: Intelligent deduplication preserving data integrity\n",
    "3. **Standardization**: Consistent formatting and validation\n",
    "4. **Data Type Optimization**: Appropriate data types for analysis and storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_royalty_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning for royalty transactions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Raw royalty data with quality issues\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataset\n",
    "    dict\n",
    "        Cleaning log with operations performed\n",
    "    \"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    cleaning_log = {\n",
    "        'initial_records': len(df),\n",
    "        'operations': []\n",
    "    }\n",
    "    \n",
    "    print(\"Starting comprehensive data cleaning...\")\n",
    "    \n",
    "    # 1. Remove exact duplicates\n",
    "    print(\"\\n1. Removing Duplicates:\")\n",
    "    initial_count = len(df_clean)\n",
    "    \n",
    "    # First, handle transaction_id duplicates (keep only non-DUP versions)\n",
    "    dup_txn_mask = df_clean['transaction_id'].str.contains('_DUP', na=False)\n",
    "    n_dup_txn = dup_txn_mask.sum()\n",
    "    df_clean = df_clean[~dup_txn_mask]\n",
    "    print(f\"   → Removed {n_dup_txn:,} records with duplicate transaction IDs\")\n",
    "    \n",
    "    # Then remove exact content duplicates (excluding transaction_id)\n",
    "    cols_for_dup_check = [col for col in df_clean.columns if col != 'transaction_id']\n",
    "    before_dup_removal = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=cols_for_dup_check, keep='first')\n",
    "    content_dups_removed = before_dup_removal - len(df_clean)\n",
    "    print(f\"   → Removed {content_dups_removed:,} content duplicate records\")\n",
    "    \n",
    "    total_duplicates_removed = initial_count - len(df_clean)\n",
    "    cleaning_log['operations'].append(f\"Removed {total_duplicates_removed:,} duplicate records\")\n",
    "    \n",
    "    # 2. Clean and standardize text fields\n",
    "    print(\"\\n2. Standardizing Text Fields:\")\n",
    "    \n",
    "    # Clean artist names\n",
    "    if 'artist_name' in df_clean.columns:\n",
    "        # Fix case issues and trim whitespace\n",
    "        df_clean['artist_name'] = df_clean['artist_name'].str.strip()\n",
    "        df_clean['artist_name'] = df_clean['artist_name'].str.title()\n",
    "        print(f\"   → Standardized artist names (title case, trimmed whitespace)\")\n",
    "    \n",
    "    # Clean song titles\n",
    "    if 'song_title' in df_clean.columns:\n",
    "        df_clean['song_title'] = df_clean['song_title'].str.strip()\n",
    "        df_clean['song_title'] = df_clean['song_title'].str.title()\n",
    "        print(f\"   → Standardized song titles (title case, trimmed whitespace)\")\n",
    "    \n",
    "    # 3. Handle missing values with business logic\n",
    "    print(\"\\n3. Handling Missing Values:\")\n",
    "    \n",
    "    # Artist name imputation (use artist_id mapping)\n",
    "    artist_name_missing = df_clean['artist_name'].isnull().sum()\n",
    "    if artist_name_missing > 0:\n",
    "        # Create artist_id to artist_name mapping from non-null values\n",
    "        artist_mapping = df_clean.dropna(subset=['artist_name']).groupby('artist_id')['artist_name'].first()\n",
    "        \n",
    "        # Fill missing artist names\n",
    "        mask = df_clean['artist_name'].isnull()\n",
    "        df_clean.loc[mask, 'artist_name'] = df_clean.loc[mask, 'artist_id'].map(artist_mapping)\n",
    "        \n",
    "        filled_count = artist_name_missing - df_clean['artist_name'].isnull().sum()\n",
    "        print(f\"   → Filled {filled_count:,} missing artist names using artist_id mapping\")\n",
    "    \n",
    "    # Song title imputation\n",
    "    song_title_missing = df_clean['song_title'].isnull().sum()\n",
    "    if song_title_missing > 0:\n",
    "        # For missing song titles, use a pattern based on song_id\n",
    "        mask = df_clean['song_title'].isnull()\n",
    "        df_clean.loc[mask, 'song_title'] = df_clean.loc[mask, 'song_id'].apply(lambda x: f\"Unknown Song {x}\")\n",
    "        print(f\"   → Filled {song_title_missing:,} missing song titles with placeholder pattern\")\n",
    "    \n",
    "    # Payment status imputation (default to most common status)\n",
    "    payment_status_missing = df_clean['payment_status'].isnull().sum()\n",
    "    if payment_status_missing > 0:\n",
    "        most_common_status = df_clean['payment_status'].mode()[0]\n",
    "        df_clean['payment_status'].fillna(most_common_status, inplace=True)\n",
    "        print(f\"   → Filled {payment_status_missing:,} missing payment statuses with '{most_common_status}'\")\n",
    "    \n",
    "    # Region imputation (default to 'Global' for missing regions)\n",
    "    region_missing = df_clean['region'].isnull().sum()\n",
    "    if region_missing > 0:\n",
    "        df_clean['region'].fillna('Global', inplace=True)\n",
    "        print(f\"   → Filled {region_missing:,} missing regions with 'Global'\")\n",
    "    \n",
    "    # Royalty amount handling (critical field - flag but don't impute)\n",
    "    royalty_missing = df_clean['royalty_amount'].isnull().sum()\n",
    "    if royalty_missing > 0:\n",
    "        print(f\"   WARNING: Found {royalty_missing:,} missing royalty amounts - flagging for manual review\")\n",
    "        df_clean['needs_review'] = df_clean['royalty_amount'].isnull()\n",
    "        \n",
    "        # For analysis purposes, temporarily set to 0 (but flag for review)\n",
    "        df_clean['royalty_amount'].fillna(0, inplace=True)\n",
    "    else:\n",
    "        df_clean['needs_review'] = False\n",
    "    \n",
    "    # 4. Data type optimization\n",
    "    print(\"\\n4. Optimizing Data Types:\")\n",
    "    \n",
    "    # Convert date column\n",
    "    if df_clean['distribution_date'].dtype == 'object':\n",
    "        df_clean['distribution_date'] = pd.to_datetime(df_clean['distribution_date'])\n",
    "        print(f\"   → Converted distribution_date to datetime\")\n",
    "    \n",
    "    # Optimize categorical columns\n",
    "    categorical_cols = ['artist_id', 'song_id', 'payment_status', 'licensing_channel', 'region']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype('category')\n",
    "            print(f\"   → Converted {col} to category\")\n",
    "    \n",
    "    # Ensure numeric precision for royalty amounts\n",
    "    df_clean['royalty_amount'] = pd.to_numeric(df_clean['royalty_amount'], errors='coerce').round(2)\n",
    "    \n",
    "    # 5. Add derived fields for analysis\n",
    "    print(\"\\n5. Adding Derived Fields:\")\n",
    "    \n",
    "    # Extract year and month for temporal analysis\n",
    "    df_clean['year'] = df_clean['distribution_date'].dt.year\n",
    "    df_clean['month'] = df_clean['distribution_date'].dt.month\n",
    "    df_clean['quarter'] = df_clean['distribution_date'].dt.quarter\n",
    "    print(f\"   → Added temporal fields: year, month, quarter\")\n",
    "    \n",
    "    # Add royalty amount categories\n",
    "    df_clean['royalty_tier'] = pd.cut(df_clean['royalty_amount'], \n",
    "                                    bins=[0, 10, 50, 200, 1000, float('inf')],\n",
    "                                    labels=['Micro', 'Small', 'Medium', 'Large', 'Premium'])\n",
    "    print(f\"   → Added royalty tier categorization\")\n",
    "    \n",
    "    # Final statistics\n",
    "    cleaning_log['final_records'] = len(df_clean)\n",
    "    cleaning_log['records_removed'] = cleaning_log['initial_records'] - cleaning_log['final_records']\n",
    "    cleaning_log['missing_after_cleaning'] = df_clean.isnull().sum().sum()\n",
    "    \n",
    "    print(f\"\\n Data Cleaning Complete:\")\n",
    "    print(f\"   • Initial records: {cleaning_log['initial_records']:,}\")\n",
    "    print(f\"   • Final records: {cleaning_log['final_records']:,}\")\n",
    "    print(f\"   • Records removed: {cleaning_log['records_removed']:,}\")\n",
    "    print(f\"   • Remaining missing values: {cleaning_log['missing_after_cleaning']:,}\")\n",
    "    \n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "# Apply comprehensive cleaning\n",
    "df_cleaned, cleaning_log = clean_royalty_data(df_dirty)\n",
    "\n",
    "# Profile the cleaned dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "profile_clean = comprehensive_data_profile(df_cleaned, \"Post-Cleaning Data Profile\")\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Anomaly Detection\n",
    "\n",
    "### 5.1 Statistical Outlier Detection\n",
    "\n",
    "Anomaly detection in royalty payments helps identify:\n",
    "- **Payment Errors**: Unusually high or low royalty amounts\n",
    "- **Fraud Detection**: Suspicious payment patterns\n",
    "- **System Issues**: Data entry errors or processing problems\n",
    "- **Business Opportunities**: High-value transactions requiring attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_royalty_anomalies(df, methods=['zscore', 'iqr', 'isolation_forest']):\n",
    "    \"\"\"\n",
    "    Comprehensive anomaly detection for royalty payments\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned royalty dataset\n",
    "    methods : list\n",
    "        List of detection methods to apply\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataset with anomaly flags\n",
    "    dict\n",
    "        Anomaly detection results\n",
    "    \"\"\"\n",
    "    \n",
    "    df_anomaly = df.copy()\n",
    "    anomaly_results = {}\n",
    "    \n",
    "    print(\"Performing Anomaly Detection...\")\n",
    "    \n",
    "    # Exclude zero royalty amounts from anomaly detection (flagged records)\n",
    "    analysis_mask = (df_anomaly['royalty_amount'] > 0) & (~df_anomaly['needs_review'])\n",
    "    analysis_data = df_anomaly[analysis_mask]['royalty_amount']\n",
    "    \n",
    "    print(f\"   • Analyzing {len(analysis_data):,} valid royalty transactions\")\n",
    "    \n",
    "    # Initialize anomaly flags\n",
    "    df_anomaly['anomaly_zscore'] = False\n",
    "    df_anomaly['anomaly_iqr'] = False\n",
    "    df_anomaly['anomaly_isolation'] = False\n",
    "    df_anomaly['anomaly_composite'] = False\n",
    "    \n",
    "    # 1. Z-Score Method (Statistical outliers)\n",
    "    if 'zscore' in methods:\n",
    "        print(\"\\nZ-Score Anomaly Detection:\")\n",
    "        \n",
    "        # Calculate z-scores for royalty amounts\n",
    "        z_scores = np.abs(stats.zscore(analysis_data))\n",
    "        zscore_threshold = 3.0  # Standard threshold for outliers\n",
    "        \n",
    "        # Identify outliers\n",
    "        zscore_outliers_idx = analysis_data.index[z_scores > zscore_threshold]\n",
    "        df_anomaly.loc[zscore_outliers_idx, 'anomaly_zscore'] = True\n",
    "        \n",
    "        n_zscore_outliers = len(zscore_outliers_idx)\n",
    "        zscore_pct = (n_zscore_outliers / len(analysis_data)) * 100\n",
    "        \n",
    "        print(f\"   → Found {n_zscore_outliers:,} outliers ({zscore_pct:.2f}%)\")\n",
    "        print(f\"   → Threshold: |z-score| > {zscore_threshold}\")\n",
    "        \n",
    "        anomaly_results['zscore'] = {\n",
    "            'method': 'Z-Score',\n",
    "            'threshold': zscore_threshold,\n",
    "            'outliers': n_zscore_outliers,\n",
    "            'percentage': zscore_pct\n",
    "        }\n",
    "    \n",
    "    # 2. IQR Method (Interquartile Range)\n",
    "    if 'iqr' in methods:\n",
    "        print(\"\\nIQR Anomaly Detection:\")\n",
    "        \n",
    "        # Calculate IQR boundaries\n",
    "        Q1 = analysis_data.quantile(0.25)\n",
    "        Q3 = analysis_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier boundaries\n",
    "        iqr_multiplier = 1.5  # Standard IQR multiplier\n",
    "        lower_bound = Q1 - iqr_multiplier * IQR\n",
    "        upper_bound = Q3 + iqr_multiplier * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        iqr_outliers_mask = (analysis_data < lower_bound) | (analysis_data > upper_bound)\n",
    "        iqr_outliers_idx = analysis_data.index[iqr_outliers_mask]\n",
    "        df_anomaly.loc[iqr_outliers_idx, 'anomaly_iqr'] = True\n",
    "        \n",
    "        n_iqr_outliers = len(iqr_outliers_idx)\n",
    "        iqr_pct = (n_iqr_outliers / len(analysis_data)) * 100\n",
    "        \n",
    "        print(f\"   → Found {n_iqr_outliers:,} outliers ({iqr_pct:.2f}%)\")\n",
    "        print(f\"   → Bounds: ${lower_bound:.2f} - ${upper_bound:.2f}\")\n",
    "        print(f\"   → IQR: ${IQR:.2f} (Q1: ${Q1:.2f}, Q3: ${Q3:.2f})\")\n",
    "        \n",
    "        anomaly_results['iqr'] = {\n",
    "            'method': 'IQR',\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outliers': n_iqr_outliers,\n",
    "            'percentage': iqr_pct\n",
    "        }\n",
    "    \n",
    "    # 3. Isolation Forest (Machine Learning approach)\n",
    "    if 'isolation_forest' in methods:\n",
    "        print(\"\\nIsolation Forest Anomaly Detection:\")\n",
    "        \n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        \n",
    "        # Prepare features for isolation forest\n",
    "        features = ['royalty_amount']\n",
    "        \n",
    "        # Add additional features if available\n",
    "        if 'year' in df_anomaly.columns:\n",
    "            features.extend(['year', 'month', 'quarter'])\n",
    "        \n",
    "        # Encode categorical features\n",
    "        feature_data = df_anomaly[analysis_mask][features].copy()\n",
    "        \n",
    "        # Initialize and fit Isolation Forest\n",
    "        iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        predictions = iso_forest.fit_predict(feature_data)\n",
    "        \n",
    "        # Identify outliers (predictions = -1)\n",
    "        iso_outliers_idx = analysis_data.index[predictions == -1]\n",
    "        df_anomaly.loc[iso_outliers_idx, 'anomaly_isolation'] = True\n",
    "        \n",
    "        n_iso_outliers = len(iso_outliers_idx)\n",
    "        iso_pct = (n_iso_outliers / len(analysis_data)) * 100\n",
    "        \n",
    "        print(f\"   → Found {n_iso_outliers:,} outliers ({iso_pct:.2f}%)\")\n",
    "        print(f\"   → Contamination rate: 5%\")\n",
    "        \n",
    "        anomaly_results['isolation_forest'] = {\n",
    "            'method': 'Isolation Forest',\n",
    "            'outliers': n_iso_outliers,\n",
    "            'percentage': iso_pct\n",
    "        }\n",
    "    \n",
    "    # 4. Composite Anomaly Score\n",
    "    print(\"\\nComposite Anomaly Analysis:\")\n",
    "    \n",
    "    # Create composite anomaly flag (any method detected anomaly)\n",
    "    anomaly_cols = [col for col in df_anomaly.columns if col.startswith('anomaly_') and col != 'anomaly_composite']\n",
    "    df_anomaly['anomaly_composite'] = df_anomaly[anomaly_cols].any(axis=1)\n",
    "    \n",
    "    # Calculate anomaly scores\n",
    "    df_anomaly['anomaly_score'] = df_anomaly[anomaly_cols].sum(axis=1)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_anomalies = df_anomaly['anomaly_composite'].sum()\n",
    "    anomaly_pct = (total_anomalies / len(df_anomaly)) * 100\n",
    "    \n",
    "    print(f\"   → Total unique anomalies: {total_anomalies:,} ({anomaly_pct:.2f}%)\")\n",
    "    \n",
    "    # High-confidence anomalies (detected by multiple methods)\n",
    "    high_conf_anomalies = (df_anomaly['anomaly_score'] >= 2).sum()\n",
    "    high_conf_pct = (high_conf_anomalies / len(df_anomaly)) * 100\n",
    "    \n",
    "    print(f\"   → High-confidence anomalies: {high_conf_anomalies:,} ({high_conf_pct:.2f}%)\")\n",
    "    \n",
    "    anomaly_results['composite'] = {\n",
    "        'total_anomalies': total_anomalies,\n",
    "        'percentage': anomaly_pct,\n",
    "        'high_confidence': high_conf_anomalies,\n",
    "        'high_conf_percentage': high_conf_pct\n",
    "    }\n",
    "    \n",
    "    return df_anomaly, anomaly_results\n",
    "\n",
    "# Apply anomaly detection\n",
    "df_with_anomalies, anomaly_results = detect_royalty_anomalies(df_cleaned)\n",
    "\n",
    "# Display anomaly summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANOMALY DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method, results in anomaly_results.items():\n",
    "    print(f\"\\n{results['method']}:\")\n",
    "    print(f\"   • Outliers detected: {results['outliers']:,}\")\n",
    "    print(f\"   • Percentage: {results['percentage']:.2f}%\")\n",
    "\n",
    "# Show sample anomalies\n",
    "print(\"\\n Sample High-Value Anomalies:\")\n",
    "high_value_anomalies = df_with_anomalies[\n",
    "    (df_with_anomalies['anomaly_composite']) & \n",
    "    (df_with_anomalies['royalty_amount'] > 500)\n",
    "].sort_values('royalty_amount', ascending=False).head(10)\n",
    "\n",
    "print(high_value_anomalies[['transaction_id', 'artist_name', 'song_title', 'royalty_amount', \n",
    "                          'licensing_channel', 'region', 'anomaly_score']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Key Business Metrics\n",
    "\n",
    "### 6.1 Revenue Analytics\n",
    "\n",
    "Calculate essential business metrics for stakeholder reporting and strategic decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_business_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive business metrics for royalty data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Cleaned royalty dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing various business metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    print(\"Calculating Key Business Metrics...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Overall Portfolio Metrics\n",
    "    print(\"\\n Overall Portfolio Performance:\")\n",
    "    \n",
    "    total_royalties = df['royalty_amount'].sum()\n",
    "    total_transactions = len(df)\n",
    "    avg_royalty = df['royalty_amount'].mean()\n",
    "    median_royalty = df['royalty_amount'].median()\n",
    "    \n",
    "    print(f\"   • Total Royalties: ${total_royalties:,.2f}\")\n",
    "    print(f\"   • Total Transactions: {total_transactions:,}\")\n",
    "    print(f\"   • Average Royalty: ${avg_royalty:.2f}\")\n",
    "    print(f\"   • Median Royalty: ${median_royalty:.2f}\")\n",
    "    \n",
    "    metrics['portfolio'] = {\n",
    "        'total_royalties': total_royalties,\n",
    "        'total_transactions': total_transactions,\n",
    "        'average_royalty': avg_royalty,\n",
    "        'median_royalty': median_royalty\n",
    "    }\n",
    "    \n",
    "    # Artist Performance Metrics\n",
    "    print(\"\\nTop Artists by Total Royalties:\")\n",
    "    \n",
    "    artist_metrics = df.groupby('artist_name').agg({\n",
    "        'royalty_amount': ['sum', 'count', 'mean'],\n",
    "        'song_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    artist_metrics.columns = ['Total_Royalties', 'Transactions', 'Avg_Royalty', 'Unique_Songs']\n",
    "    artist_metrics = artist_metrics.sort_values('Total_Royalties', ascending=False)\n",
    "    \n",
    "    top_artists = artist_metrics.head(10)\n",
    "    print(top_artists.to_string())\n",
    "    \n",
    "    metrics['top_artists'] = top_artists.to_dict('index')\n",
    "    \n",
    "    # Licensing Channel Analysis\n",
    "    print(\"\\n\\nRevenue by Licensing Channel:\")\n",
    "    \n",
    "    channel_metrics = df.groupby('licensing_channel').agg({\n",
    "        'royalty_amount': ['sum', 'count', 'mean'],\n",
    "        'artist_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    channel_metrics.columns = ['Total_Revenue', 'Transactions', 'Avg_Royalty', 'Unique_Artists']\n",
    "    channel_metrics['Revenue_Share_%'] = (channel_metrics['Total_Revenue'] / total_royalties * 100).round(2)\n",
    "    channel_metrics = channel_metrics.sort_values('Total_Revenue', ascending=False)\n",
    "    \n",
    "    print(channel_metrics.to_string())\n",
    "    \n",
    "    metrics['channels'] = channel_metrics.to_dict('index')\n",
    "    \n",
    "    # Regional Analysis\n",
    "    print(\"\\n\\nRevenue by Region:\")\n",
    "    \n",
    "    region_metrics = df.groupby('region').agg({\n",
    "        'royalty_amount': ['sum', 'count', 'mean'],\n",
    "        'artist_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    region_metrics.columns = ['Total_Revenue', 'Transactions', 'Avg_Royalty', 'Unique_Artists']\n",
    "    region_metrics['Revenue_Share_%'] = (region_metrics['Total_Revenue'] / total_royalties * 100).round(2)\n",
    "    region_metrics = region_metrics.sort_values('Total_Revenue', ascending=False)\n",
    "    \n",
    "    print(region_metrics.to_string())\n",
    "    \n",
    "    metrics['regions'] = region_metrics.to_dict('index')\n",
    "    \n",
    "    # Temporal Analysis\n",
    "    print(\"\\n\\nRevenue Trends by Quarter:\")\n",
    "    \n",
    "    temporal_metrics = df.groupby(['year', 'quarter']).agg({\n",
    "        'royalty_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    temporal_metrics.columns = ['Total_Revenue', 'Transactions']\n",
    "    temporal_metrics['Period'] = temporal_metrics.index.map(lambda x: f\"{x[0]}-Q{x[1]}\")\n",
    "    \n",
    "    print(temporal_metrics.to_string())\n",
    "    \n",
    "    metrics['temporal'] = temporal_metrics.to_dict('index')\n",
    "    \n",
    "    # Payment Status Analysis\n",
    "    print(\"\\n\\nPayment Status Distribution:\")\n",
    "    \n",
    "    payment_metrics = df.groupby('payment_status').agg({\n",
    "        'royalty_amount': ['sum', 'count'],\n",
    "        'transaction_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    payment_metrics.columns = ['Total_Amount', 'Count', 'Transactions']\n",
    "    payment_metrics['Percentage_%'] = (payment_metrics['Transactions'] / total_transactions * 100).round(2)\n",
    "    \n",
    "    print(payment_metrics.to_string())\n",
    "    \n",
    "    metrics['payment_status'] = payment_metrics.to_dict('index')\n",
    "    \n",
    "    # High-Value Transaction Analysis\n",
    "    print(\"\\n\\nHigh-Value Transactions (>$1000):\")\n",
    "    \n",
    "    high_value = df[df['royalty_amount'] > 1000]\n",
    "    if len(high_value) > 0:\n",
    "        print(f\"   • Count: {len(high_value):,}\")\n",
    "        print(f\"   • Total Value: ${high_value['royalty_amount'].sum():,.2f}\")\n",
    "        print(f\"   • Percentage of Portfolio: {(len(high_value) / total_transactions * 100):.2f}%\")\n",
    "        print(f\"   • Revenue Contribution: {(high_value['royalty_amount'].sum() / total_royalties * 100):.2f}%\")\n",
    "        \n",
    "        metrics['high_value'] = {\n",
    "            'count': len(high_value),\n",
    "            'total_value': high_value['royalty_amount'].sum(),\n",
    "            'percentage': len(high_value) / total_transactions * 100,\n",
    "            'revenue_contribution': high_value['royalty_amount'].sum() / total_royalties * 100\n",
    "        }\n",
    "    else:\n",
    "        print(\"   • No high-value transactions found\")\n",
    "        metrics['high_value'] = {'count': 0}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate all business metrics\n",
    "business_metrics = calculate_business_metrics(df_with_anomalies)\n",
    "\n",
    "# Additional quick statistics\n",
    "print(\"\\n\\nQuick Performance Indicators:\")\n",
    "print(f\"   • Revenue per Artist: ${business_metrics['portfolio']['total_royalties'] / df_with_anomalies['artist_id'].nunique():.2f}\")\n",
    "print(f\"   • Revenue per Song: ${business_metrics['portfolio']['total_royalties'] / df_with_anomalies['song_id'].nunique():.2f}\")\n",
    "print(f\"   • Active Artists: {df_with_anomalies['artist_id'].nunique():,}\")\n",
    "print(f\"   • Unique Songs: {df_with_anomalies['song_id'].nunique():,}\")\n",
    "print(f\"   • Average Royalty per Transaction: ${business_metrics['portfolio']['average_royalty']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Interactive Business Intelligence Visualizations\n",
    "\n",
    "### 7.1 Power BI-Style Dashboard\n",
    "\n",
    "Creating comprehensive visualizations to support executive decision-making and operational insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the visualization theme\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Revenue Distribution by Licensing Channel (Top Left)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "channel_data = df_with_anomalies.groupby('licensing_channel')['royalty_amount'].sum().sort_values(ascending=True)\n",
    "bars = ax1.barh(channel_data.index, channel_data.values, color=sns.color_palette(\"viridis\", len(channel_data)))\n",
    "ax1.set_title('Total Revenue by Licensing Channel', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Revenue ($)')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (index, value) in enumerate(channel_data.items()):\n",
    "    ax1.text(value + max(channel_data) * 0.01, i, f'${value:,.0f}', \n",
    "             va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 2. Top 15 Artists by Revenue (Top Right)\n",
    "ax2 = fig.add_subplot(gs[0, 2:])\n",
    "top_artists = df_with_anomalies.groupby('artist_name')['royalty_amount'].sum().nlargest(15)\n",
    "ax2.bar(range(len(top_artists)), top_artists.values, color=sns.color_palette(\"plasma\", len(top_artists)))\n",
    "ax2.set_title('Top 15 Artists by Total Revenue', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Revenue ($)')\n",
    "ax2.set_xticks(range(len(top_artists)))\n",
    "ax2.set_xticklabels(top_artists.index, rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "# 3. Regional Revenue Distribution (Middle Left)\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "region_data = df_with_anomalies.groupby('region')['royalty_amount'].sum()\n",
    "colors = sns.color_palette(\"Set3\", len(region_data))\n",
    "wedges, texts, autotexts = ax3.pie(region_data.values, labels=region_data.index, autopct='%1.1f%%',\n",
    "                                   colors=colors, startangle=90)\n",
    "ax3.set_title('Revenue Distribution by Region', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Payment Status Distribution (Middle Right)\n",
    "ax4 = fig.add_subplot(gs[1, 2:])\n",
    "payment_data = df_with_anomalies.groupby('payment_status').size()\n",
    "ax4.bar(payment_data.index, payment_data.values, color=sns.color_palette(\"coolwarm\", len(payment_data)))\n",
    "ax4.set_title('Transaction Count by Payment Status', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Number of Transactions')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Royalty Amount Distribution (Bottom Left)\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "royalty_amounts = df_with_anomalies[df_with_anomalies['royalty_amount'] > 0]['royalty_amount']\n",
    "ax5.hist(royalty_amounts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax5.set_title('Distribution of Royalty Amounts', fontsize=14, fontweight='bold')\n",
    "ax5.set_xlabel('Royalty Amount ($)')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_yscale('log')\n",
    "\n",
    "# 6. Monthly Revenue Trends (Bottom Right)\n",
    "ax6 = fig.add_subplot(gs[2, 2:])\n",
    "monthly_revenue = df_with_anomalies.groupby([df_with_anomalies['distribution_date'].dt.to_period('M')])['royalty_amount'].sum()\n",
    "ax6.plot(range(len(monthly_revenue)), monthly_revenue.values, marker='o', linewidth=2, markersize=4)\n",
    "ax6.set_title('Monthly Revenue Trends', fontsize=14, fontweight='bold')\n",
    "ax6.set_xlabel('Month')\n",
    "ax6.set_ylabel('Revenue ($)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Anomaly Detection Results (Full Width Bottom)\n",
    "ax7 = fig.add_subplot(gs[3, :])\n",
    "anomaly_summary = pd.Series({\n",
    "    'Z-Score Outliers': (df_with_anomalies['anomaly_zscore']).sum(),\n",
    "    'IQR Outliers': (df_with_anomalies['anomaly_iqr']).sum(),\n",
    "    'Isolation Forest': (df_with_anomalies['anomaly_isolation']).sum(),\n",
    "    'High Confidence': (df_with_anomalies['anomaly_score'] >= 2).sum()\n",
    "})\n",
    "\n",
    "bars = ax7.bar(anomaly_summary.index, anomaly_summary.values, \n",
    "               color=['red', 'orange', 'purple', 'darkred'], alpha=0.7)\n",
    "ax7.set_title('Anomaly Detection Results by Method', fontsize=14, fontweight='bold')\n",
    "ax7.set_ylabel('Number of Anomalies Detected')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height + max(anomaly_summary) * 0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Artist Royalty Analytics Dashboard - Executive Summary', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Dashboard Generated Successfully!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 7.2 Detailed Analysis Visualizations\n",
    "\n",
    "Additional focused visualizations for deep-dive analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed analysis visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Detailed Royalty Analytics - Deep Dive Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Box Plot: Royalty Distribution by Channel\n",
    "ax1 = axes[0, 0]\n",
    "channel_order = df_with_anomalies.groupby('licensing_channel')['royalty_amount'].median().sort_values(ascending=False).index\n",
    "sns.boxplot(data=df_with_anomalies, y='licensing_channel', x='royalty_amount', \n",
    "            order=channel_order, ax=ax1, palette='Set2')\n",
    "ax1.set_title('Royalty Distribution by Licensing Channel', fontweight='bold')\n",
    "ax1.set_xlabel('Royalty Amount ($)')\n",
    "ax1.set_xlim(0, 1000)  # Focus on main distribution\n",
    "\n",
    "# 2. Heatmap: Revenue by Region and Channel\n",
    "ax2 = axes[0, 1]\n",
    "pivot_data = df_with_anomalies.pivot_table(values='royalty_amount', \n",
    "                                          index='region', \n",
    "                                          columns='licensing_channel', \n",
    "                                          aggfunc='sum', \n",
    "                                          fill_value=0)\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Revenue ($)'})\n",
    "ax2.set_title('Revenue Heatmap: Region vs Channel', fontweight='bold')\n",
    "ax2.set_xlabel('Licensing Channel')\n",
    "ax2.set_ylabel('Region')\n",
    "\n",
    "# 3. Time Series: Monthly Revenue Trends\n",
    "ax3 = axes[0, 2]\n",
    "monthly_data = df_with_anomalies.groupby(df_with_anomalies['distribution_date'].dt.to_period('M')).agg({\n",
    "    'royalty_amount': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "})\n",
    "\n",
    "# Plot both revenue and transaction count\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(range(len(monthly_data)), monthly_data['royalty_amount'], \n",
    "                'b-o', label='Revenue', linewidth=2, markersize=4)\n",
    "line2 = ax3_twin.plot(range(len(monthly_data)), monthly_data['transaction_id'], \n",
    "                     'r-s', label='Transactions', linewidth=2, markersize=4)\n",
    "\n",
    "ax3.set_title('Monthly Revenue and Transaction Trends', fontweight='bold')\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Revenue ($)', color='blue')\n",
    "ax3_twin.set_ylabel('Transaction Count', color='red')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "# 4. Scatter Plot: Artist Performance (Revenue vs Transaction Count)\n",
    "ax4 = axes[1, 0]\n",
    "artist_performance = df_with_anomalies.groupby('artist_name').agg({\n",
    "    'royalty_amount': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "scatter = ax4.scatter(artist_performance['transaction_id'], \n",
    "                     artist_performance['royalty_amount'],\n",
    "                     alpha=0.6, s=50, c=artist_performance['royalty_amount'], \n",
    "                     cmap='viridis')\n",
    "ax4.set_title('Artist Performance: Revenue vs Transaction Volume', fontweight='bold')\n",
    "ax4.set_xlabel('Number of Transactions')\n",
    "ax4.set_ylabel('Total Revenue ($)')\n",
    "plt.colorbar(scatter, ax=ax4, label='Revenue ($)')\n",
    "\n",
    "# 5. Payment Status Timeline\n",
    "ax5 = axes[1, 1]\n",
    "payment_timeline = df_with_anomalies.groupby([\n",
    "    df_with_anomalies['distribution_date'].dt.to_period('Q'),\n",
    "    'payment_status'\n",
    "])['royalty_amount'].sum().unstack(fill_value=0)\n",
    "\n",
    "payment_timeline.plot(kind='bar', stacked=True, ax=ax5, \n",
    "                     color=sns.color_palette(\"Set2\", len(payment_timeline.columns)))\n",
    "ax5.set_title('Payment Status Distribution Over Time', fontweight='bold')\n",
    "ax5.set_xlabel('Quarter')\n",
    "ax5.set_ylabel('Revenue ($)')\n",
    "ax5.legend(title='Payment Status', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Royalty Tier Analysis\n",
    "ax6 = axes[1, 2]\n",
    "tier_data = df_with_anomalies['royalty_tier'].value_counts()\n",
    "colors = sns.color_palette(\"viridis\", len(tier_data))\n",
    "bars = ax6.bar(tier_data.index, tier_data.values, color=colors)\n",
    "ax6.set_title('Transaction Distribution by Royalty Tier', fontweight='bold')\n",
    "ax6.set_xlabel('Royalty Tier')\n",
    "ax6.set_ylabel('Number of Transactions')\n",
    "\n",
    "# Add percentage labels\n",
    "total_transactions = len(df_with_anomalies)\n",
    "for bar, count in zip(bars, tier_data.values):\n",
    "    percentage = (count / total_transactions) * 100\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() + total_transactions * 0.01,\n",
    "             f'{percentage:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create an additional anomaly-focused visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Anomaly Analysis Dashboard\n",
    "gs2 = plt.GridSpec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Anomaly Distribution by Channel\n",
    "ax1 = plt.subplot(gs2[0, 0])\n",
    "anomaly_by_channel = df_with_anomalies[df_with_anomalies['anomaly_composite']].groupby('licensing_channel').size()\n",
    "ax1.bar(anomaly_by_channel.index, anomaly_by_channel.values, color='red', alpha=0.7)\n",
    "ax1.set_title('Anomalies by Licensing Channel', fontweight='bold')\n",
    "ax1.set_ylabel('Number of Anomalies')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Anomaly Score Distribution\n",
    "ax2 = plt.subplot(gs2[0, 1])\n",
    "anomaly_scores = df_with_anomalies['anomaly_score']\n",
    "ax2.hist(anomaly_scores, bins=range(5), alpha=0.7, color='orange', edgecolor='black')\n",
    "ax2.set_title('Distribution of Anomaly Scores', fontweight='bold')\n",
    "ax2.set_xlabel('Anomaly Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "# 3. High-Value Anomalies\n",
    "ax3 = plt.subplot(gs2[0, 2])\n",
    "high_value_anomalies = df_with_anomalies[\n",
    "    (df_with_anomalies['anomaly_composite']) & \n",
    "    (df_with_anomalies['royalty_amount'] > 200)\n",
    "]\n",
    "if len(high_value_anomalies) > 0:\n",
    "    ax3.scatter(high_value_anomalies['royalty_amount'], \n",
    "               high_value_anomalies['anomaly_score'],\n",
    "               alpha=0.7, s=60, c='red')\n",
    "    ax3.set_title('High-Value Anomalies', fontweight='bold')\n",
    "    ax3.set_xlabel('Royalty Amount ($)')\n",
    "    ax3.set_ylabel('Anomaly Score')\n",
    "\n",
    "# 4. Temporal Anomaly Pattern\n",
    "ax4 = plt.subplot(gs2[1, :])\n",
    "anomaly_timeline = df_with_anomalies[df_with_anomalies['anomaly_composite']].groupby(\n",
    "    df_with_anomalies['distribution_date'].dt.to_period('M')\n",
    ").size()\n",
    "\n",
    "if len(anomaly_timeline) > 0:\n",
    "    ax4.plot(range(len(anomaly_timeline)), anomaly_timeline.values, \n",
    "            'ro-', linewidth=2, markersize=6, alpha=0.7)\n",
    "    ax4.set_title('Anomaly Detection Timeline', fontweight='bold')\n",
    "    ax4.set_xlabel('Month')\n",
    "    ax4.set_ylabel('Number of Anomalies')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Anomaly Detection Deep Dive Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Detailed visualizations completed!\")\n",
    "print(\"All charts provide actionable business intelligence insights\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Data Quality Documentation & Process Summary\n",
    "\n",
    "### 8.1 Data Sources & Methodology Documentation\n",
    "\n",
    "This section provides comprehensive documentation of all processes, decisions, and findings for audit purposes and future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive documentation report\n",
    "def generate_documentation_report():\n",
    "    \"\"\"\n",
    "    Generate comprehensive documentation for the data quality project\n",
    "    \"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'project_summary': {},\n",
    "        'data_sources': {},\n",
    "        'cleaning_procedures': {},\n",
    "        'anomaly_detection': {},\n",
    "        'business_insights': {},\n",
    "        'recommendations': {}\n",
    "    }\n",
    "    \n",
    "    print(\"COMPREHENSIVE PROJECT DOCUMENTATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Project Summary\n",
    "    print(\"\\nPROJECT SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    project_summary = f\"\"\"\n",
    "    Project Name: Artist Royalty Data Quality Assessment\n",
    "    Analysis Period: {df_with_anomalies['distribution_date'].min().strftime('%Y-%m-%d')} to {df_with_anomalies['distribution_date'].max().strftime('%Y-%m-%d')}\n",
    "    Dataset Size: {len(df_with_anomalies):,} transactions\n",
    "    Artists Analyzed: {df_with_anomalies['artist_id'].nunique():,}\n",
    "    Songs Analyzed: {df_with_anomalies['song_id'].nunique():,}\n",
    "    Total Revenue: ${df_with_anomalies['royalty_amount'].sum():,.2f}\n",
    "    \"\"\"\n",
    "    print(project_summary)\n",
    "    \n",
    "    # 2. Data Sources Documentation\n",
    "    print(\"\\nDATA SOURCES & GENERATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    data_sources = f\"\"\"\n",
    "    Data Generation Method: Synthetic data using Faker library and realistic business logic\n",
    "    \n",
    "    Key Data Elements:\n",
    "    • Transaction IDs: Sequential numbering (TXN_XXXXXXXX)\n",
    "    • Artist Information: 500 unique artists with realistic names\n",
    "    • Song Catalog: 10,000 unique songs with generated titles\n",
    "    • Licensing Channels: 7 channels with industry-appropriate revenue multipliers\n",
    "    • Geographic Coverage: 7 regions with realistic distribution patterns\n",
    "    • Payment Status: 5 status types reflecting real-world payment workflows\n",
    "    \n",
    "    Business Logic Applied:\n",
    "    • Channel-specific royalty multipliers (Sync License: 8x, TV/Film: 5x, etc.)\n",
    "    • Regional distribution patterns (North America: 30%, Europe: 25%, etc.)\n",
    "    • Seasonal payment patterns across 2-year period\n",
    "    • Exponential distribution for royalty amounts (realistic for music industry)\n",
    "    \"\"\"\n",
    "    print(data_sources)\n",
    "    \n",
    "    # 3. Data Quality Issues Introduced\n",
    "    print(\"\\nDATA QUALITY ISSUES (INTENTIONALLY INTRODUCED)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    quality_issues = f\"\"\"\n",
    "    Missing Values:\n",
    "    • Target Rate: 5% across key fields\n",
    "    • Affected Columns: artist_name, song_title, royalty_amount, payment_status, region\n",
    "    • Distribution: Randomly distributed to simulate real-world conditions\n",
    "    \n",
    "    Duplicate Records:\n",
    "    • Target Rate: 2% of total records\n",
    "    • Implementation: Exact content duplicates with modified transaction_ids\n",
    "    • Purpose: Test deduplication procedures\n",
    "    \n",
    "    Formatting Issues:\n",
    "    • Case inconsistencies in artist names (1,000 records)\n",
    "    • Whitespace issues in song titles (500 records)\n",
    "    • Purpose: Test standardization procedures\n",
    "    \"\"\"\n",
    "    print(quality_issues)\n",
    "    \n",
    "    # 4. Cleaning Procedures Documentation\n",
    "    print(\"\\nDATA CLEANING PROCEDURES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    cleaning_procedures = f\"\"\"\n",
    "    Step 1: Duplicate Removal\n",
    "    • Removed {cleaning_log['records_removed']:,} duplicate records\n",
    "    • Method: Content-based deduplication (excluding transaction_id)\n",
    "    • Preserved data integrity by keeping first occurrence\n",
    "    \n",
    "    Step 2: Text Standardization\n",
    "    • Applied title case formatting to artist names and song titles\n",
    "    • Trimmed leading/trailing whitespace\n",
    "    • Maintained consistency across all text fields\n",
    "    \n",
    "    Step 3: Missing Value Treatment\n",
    "    • Artist Names: Imputed using artist_id mapping from non-null values\n",
    "    • Song Titles: Generated placeholder patterns for missing values\n",
    "    • Payment Status: Imputed using most common status ('Paid')\n",
    "    • Regions: Defaulted to 'Global' for missing values\n",
    "    • Royalty Amounts: Flagged for manual review (critical financial data)\n",
    "    \n",
    "    Step 4: Data Type Optimization\n",
    "    • Converted categorical fields to category dtype for memory efficiency\n",
    "    • Standardized date formats to datetime\n",
    "    • Ensured numeric precision for financial amounts\n",
    "    \n",
    "    Step 5: Derived Field Creation\n",
    "    • Added temporal fields: year, month, quarter\n",
    "    • Created royalty tier categorization (Micro/Small/Medium/Large/Premium)\n",
    "    • Added review flags for quality control\n",
    "    \"\"\"\n",
    "    print(cleaning_procedures)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the documentation\n",
    "documentation_report = generate_documentation_report()\n",
    "\n",
    "# 5. Anomaly Detection Documentation\n",
    "print(\"\\nANOMALY DETECTION METHODOLOGY\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "anomaly_documentation = f\"\"\"\n",
    "Methods Applied:\n",
    "\n",
    "1. Z-Score Analysis (Statistical)\n",
    "   • Threshold: |z-score| > 3.0\n",
    "   • Detected: {anomaly_results['zscore']['outliers']:,} outliers ({anomaly_results['zscore']['percentage']:.2f}%)\n",
    "   • Purpose: Identify statistical deviations from normal distribution\n",
    "\n",
    "2. Interquartile Range (IQR) Analysis\n",
    "   • Method: Q1 - 1.5*IQR to Q3 + 1.5*IQR\n",
    "   • Detected: {anomaly_results['iqr']['outliers']:,} outliers ({anomaly_results['iqr']['percentage']:.2f}%)\n",
    "   • Bounds: ${anomaly_results['iqr']['lower_bound']:.2f} to ${anomaly_results['iqr']['upper_bound']:.2f}\n",
    "\n",
    "3. Isolation Forest (Machine Learning)\n",
    "   • Contamination Rate: 5%\n",
    "   • Detected: {anomaly_results['isolation_forest']['outliers']:,} outliers ({anomaly_results['isolation_forest']['percentage']:.2f}%)\n",
    "   • Features: royalty_amount, temporal features\n",
    "\n",
    "Composite Analysis:\n",
    "• Total Unique Anomalies: {anomaly_results['composite']['total_anomalies']:,}\n",
    "• High-Confidence Anomalies: {anomaly_results['composite']['high_confidence']:,}\n",
    "• Recommended Action: Manual review for transactions with anomaly_score >= 2\n",
    "\"\"\"\n",
    "print(anomaly_documentation)\n",
    "\n",
    "# 6. Business Insights Summary\n",
    "print(\"\\n KEY BUSINESS INSIGHTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate key insights\n",
    "top_channel = max(business_metrics['channels'].items(), key=lambda x: x[1]['Total_Revenue'])\n",
    "top_region = max(business_metrics['regions'].items(), key=lambda x: x[1]['Total_Revenue'])\n",
    "top_artist = max(business_metrics['top_artists'].items(), key=lambda x: x[1]['Total_Royalties'])\n",
    "\n",
    "business_insights = f\"\"\"\n",
    "Revenue Performance:\n",
    "• Total Portfolio Value: ${business_metrics['portfolio']['total_royalties']:,.2f}\n",
    "• Average Transaction Value: ${business_metrics['portfolio']['average_royalty']:.2f}\n",
    "• Revenue Concentration: Top 10 artists represent significant portfolio share\n",
    "\n",
    "Channel Analysis:\n",
    "• Highest Revenue Channel: {top_channel[0]} (${top_channel[1]['Total_Revenue']:,.2f})\n",
    "• Most Transactions: Streaming platform dominance\n",
    "• Highest Average Royalty: Sync License and TV/Film channels\n",
    "\n",
    "Geographic Distribution:\n",
    "• Top Region: {top_region[0]} (${top_region[1]['Total_Revenue']:,.2f})\n",
    "• Regional Diversity: Revenue distributed across {len(business_metrics['regions'])} regions\n",
    "• Growth Opportunities: Emerging markets showing potential\n",
    "\n",
    "Payment Efficiency:\n",
    "• Paid Status: {df_with_anomalies[df_with_anomalies['payment_status'] == 'Paid'].shape[0]:,} transactions\n",
    "• Pending Review: {df_with_anomalies[df_with_anomalies['needs_review']].shape[0]:,} transactions\n",
    "• Processing Efficiency: {(df_with_anomalies[df_with_anomalies['payment_status'] == 'Paid'].shape[0] / len(df_with_anomalies) * 100):.1f}% completion rate\n",
    "\"\"\"\n",
    "print(business_insights)\n",
    "\n",
    "# 7. Recommendations\n",
    "print(\"\\n RECOMMENDATIONS FOR PRODUCTION IMPLEMENTATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "Data Quality Monitoring:\n",
    "1. Implement automated data quality checks for incoming royalty data\n",
    "2. Set up alerts for anomaly detection thresholds\n",
    "3. Establish regular data profiling schedules (weekly/monthly)\n",
    "4. Create data lineage documentation for audit trails\n",
    "\n",
    "Process Improvements:\n",
    "1. Standardize artist and song metadata at source systems\n",
    "2. Implement real-time duplicate detection\n",
    "3. Establish data validation rules for financial amounts\n",
    "4. Create automated reconciliation processes\n",
    "\n",
    "Business Intelligence:\n",
    "1. Deploy interactive dashboards for stakeholder access\n",
    "2. Set up automated monthly/quarterly reporting\n",
    "3. Implement predictive analytics for revenue forecasting\n",
    "4. Create artist performance scorecards\n",
    "\n",
    "Risk Management:\n",
    "1. Establish fraud detection algorithms for unusual payment patterns\n",
    "2. Implement multi-level approval workflows for high-value transactions\n",
    "3. Create contingency procedures for data quality issues\n",
    "4. Maintain backup and recovery procedures for critical data\n",
    "\n",
    "Technology Enhancements:\n",
    "1. Consider implementing real-time data streaming for faster processing\n",
    "2. Evaluate cloud-based analytics platforms for scalability\n",
    "3. Implement machine learning models for predictive insights\n",
    "4. Create APIs for seamless data integration\n",
    "\"\"\"\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Data Export & Final Deliverables\n",
    "\n",
    "### 9.1 CSV Export for Dashboarding and Reporting\n",
    "\n",
    "Export the cleaned, analyzed dataset for use in external BI tools and reporting systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for export\n",
    "def prepare_export_datasets(df):\n",
    "    \"\"\"\n",
    "    Prepare multiple dataset versions for different use cases\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Preparing Dataset Exports...\")\n",
    "    \n",
    "    # 1. Main cleaned dataset (all records)\n",
    "    df_main = df.copy()\n",
    "    \n",
    "    # Convert categorical columns back to strings for CSV compatibility\n",
    "    categorical_cols = df_main.select_dtypes(include=['category']).columns\n",
    "    for col in categorical_cols:\n",
    "        df_main[col] = df_main[col].astype(str)\n",
    "    \n",
    "    # 2. Business intelligence dataset (optimized for BI tools)\n",
    "    df_bi = df_main[[\n",
    "        'transaction_id', 'artist_id', 'artist_name', 'song_id', 'song_title',\n",
    "        'royalty_amount', 'distribution_date', 'payment_status', \n",
    "        'licensing_channel', 'region', 'year', 'quarter', 'royalty_tier'\n",
    "    ]].copy()\n",
    "    \n",
    "    # 3. Anomaly report dataset (focus on outliers)\n",
    "    df_anomalies = df_main[df_main['anomaly_composite']].copy()\n",
    "    \n",
    "    # 4. Executive summary dataset (aggregated metrics)\n",
    "    df_executive = df_main.groupby(['artist_name', 'licensing_channel', 'region']).agg({\n",
    "        'royalty_amount': ['sum', 'mean', 'count'],\n",
    "        'song_id': 'nunique',\n",
    "        'distribution_date': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names for executive summary\n",
    "    df_executive.columns = ['_'.join(col).strip() for col in df_executive.columns.values]\n",
    "    df_executive = df_executive.reset_index()\n",
    "    \n",
    "    return df_main, df_bi, df_anomalies, df_executive\n",
    "\n",
    "# Prepare all export datasets\n",
    "df_main_export, df_bi_export, df_anomalies_export, df_executive_export = prepare_export_datasets(df_with_anomalies)\n",
    "\n",
    "# Export to CSV files\n",
    "export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"Exporting Datasets to CSV...\")\n",
    "\n",
    "# Export configurations\n",
    "export_configs = [\n",
    "    {\n",
    "        'data': df_main_export,\n",
    "        'filename': f'royalty_data_cleaned_{export_timestamp}.csv',\n",
    "        'description': 'Complete cleaned dataset with all features'\n",
    "    },\n",
    "    {\n",
    "        'data': df_bi_export,\n",
    "        'filename': f'royalty_data_bi_{export_timestamp}.csv',\n",
    "        'description': 'Business Intelligence optimized dataset'\n",
    "    },\n",
    "    {\n",
    "        'data': df_anomalies_export,\n",
    "        'filename': f'royalty_anomalies_{export_timestamp}.csv',\n",
    "        'description': 'Anomaly detection results for investigation'\n",
    "    },\n",
    "    {\n",
    "        'data': df_executive_export,\n",
    "        'filename': f'royalty_executive_summary_{export_timestamp}.csv',\n",
    "        'description': 'Executive summary with aggregated metrics'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Perform exports\n",
    "for config in export_configs:\n",
    "    try:\n",
    "        config['data'].to_csv(config['filename'], index=False, encoding='utf-8')\n",
    "        file_size = len(config['data'])\n",
    "        print(f\"   {config['filename']}\")\n",
    "        print(f\"      • Description: {config['description']}\")\n",
    "        print(f\"      • Records: {file_size:,}\")\n",
    "        print(f\"      • Columns: {len(config['data'].columns)}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error exporting {config['filename']}: {str(e)}\")\n",
    "\n",
    "# Generate export metadata file\n",
    "metadata = {\n",
    "    'export_timestamp': export_timestamp,\n",
    "    'project_name': 'Artist Royalty Data Quality Project',\n",
    "    'total_records_analyzed': len(df_with_anomalies),\n",
    "    'anomalies_detected': df_with_anomalies['anomaly_composite'].sum(),\n",
    "    'data_quality_score': f\"{((len(df_with_anomalies) - df_with_anomalies['needs_review'].sum()) / len(df_with_anomalies) * 100):.1f}%\",\n",
    "    'files_exported': [config['filename'] for config in export_configs],\n",
    "    'key_metrics': {\n",
    "        'total_revenue': f\"${df_with_anomalies['royalty_amount'].sum():,.2f}\",\n",
    "        'unique_artists': df_with_anomalies['artist_id'].nunique(),\n",
    "        'unique_songs': df_with_anomalies['song_id'].nunique(),\n",
    "        'date_range': f\"{df_with_anomalies['distribution_date'].min().strftime('%Y-%m-%d')} to {df_with_anomalies['distribution_date'].max().strftime('%Y-%m-%d')}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Export metadata as JSON\n",
    "import json\n",
    "metadata_filename = f'export_metadata_{export_timestamp}.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Export metadata saved: {metadata_filename}\")\n",
    "\n",
    "# Final project summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ARTIST ROYALTY DATA QUALITY PROJECT - COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_summary = f\"\"\"\n",
    "PROJECT STATISTICS:\n",
    "   • Initial Records: {len(df_raw):,}\n",
    "   • Final Clean Records: {len(df_with_anomalies):,}\n",
    "   • Data Quality Improvement: {((len(df_with_anomalies) - df_with_anomalies['needs_review'].sum()) / len(df_with_anomalies) * 100):.1f}%\n",
    "   • Anomalies Detected: {df_with_anomalies['anomaly_composite'].sum():,}\n",
    "   • High-Confidence Anomalies: {(df_with_anomalies['anomaly_score'] >= 2).sum():,}\n",
    "\n",
    "FINANCIAL SUMMARY:\n",
    "   • Total Portfolio Value: ${df_with_anomalies['royalty_amount'].sum():,.2f}\n",
    "   • Average Transaction: ${df_with_anomalies['royalty_amount'].mean():.2f}\n",
    "   • Revenue at Risk (Flagged): ${df_with_anomalies[df_with_anomalies['needs_review']]['royalty_amount'].sum():,.2f}\n",
    "\n",
    "BUSINESS INSIGHTS:\n",
    "   • Active Artists: {df_with_anomalies['artist_id'].nunique():,}\n",
    "   • Unique Songs: {df_with_anomalies['song_id'].nunique():,}\n",
    "   • Licensing Channels: {df_with_anomalies['licensing_channel'].nunique()}\n",
    "   • Geographic Regions: {df_with_anomalies['region'].nunique()}\n",
    "\n",
    "DELIVERABLES CREATED:\n",
    "   • Cleaned datasets (4 CSV files)\n",
    "   • Comprehensive visualizations and dashboards\n",
    "   • Anomaly detection reports\n",
    "   • Process documentation\n",
    "   • Business intelligence insights\n",
    "   • Production recommendations\n",
    "\n",
    "PROJECT STATUS: COMPLETE\n",
    "   Ready for production implementation and stakeholder review.\n",
    "   \n",
    "NEXT STEPS:\n",
    "   1. Review anomaly flagged transactions\n",
    "   2. Implement recommended data quality procedures\n",
    "   3. Deploy automated monitoring systems\n",
    "   4. Schedule regular data quality assessments\n",
    "\"\"\"\n",
    "\n",
    "print(final_summary)\n",
    "\n",
    "print(\"\\nThank you for using the Artist Royalty Data Quality Assessment System!\")\n",
    "print(\"All datasets and documentation are ready for business use.\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
